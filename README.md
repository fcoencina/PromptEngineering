# Tarea 3: T√©cnicas Avanzadas de Prompt Engineering

**Asignatura:** Aprendizaje de M√°quinas
**Estudiante:** Francisco Encina
**Carrera:** Ingenier√≠a Civil Telem√°tica
**Fecha:** Noviembre 2024

---

## üìã Descripci√≥n General

Este proyecto explora y eval√∫a tres t√©cnicas avanzadas de prompt engineering: **Chain-of-Thought (CoT)**, **Tree-of-Thought (ToT)** y **Self-Refine**. El objetivo es comprender c√≥mo estas estrategias influyen en la calidad, precisi√≥n y creatividad de las respuestas generadas por modelos de lenguaje de gran escala (LLMs).

---

## üéØ Objetivos

1. Profundizar en t√©cnicas de prompt engineering mediante experimentaci√≥n pr√°ctica
2. Analizar c√≥mo diferentes estrategias afectan el razonamiento de LLMs
3. Identificar fortalezas, limitaciones y casos de uso de cada t√©cnica
4. Evaluar la confiabilidad de t√©cnicas como Self-Refine ante errores conceptuales

---

## üõ†Ô∏è Configuraci√≥n T√©cnica

### Modelo Utilizado
- **Modelo:** Mistral 7B Instruct v0.2
- **Plataforma:** Google Colab (GPU T4 gratuita)
- **Raz√≥n:** Balance entre rendimiento y recursos disponibles

### Dependencias
```bash
pip install transformers accelerate torch bitsandbytes
```

### Configuraci√≥n
- `max_tokens`: 512 (l√≠mite intencional para an√°lisis de comportamiento)
- `temperature`: 0.7
- `dtype`: float16

---

## üìÇ Estructura del Proyecto
```
‚îú‚îÄ‚îÄ README.md                          # Este archivo
‚îú‚îÄ‚îÄ PromptEngineering.ipynb     # Notebook
```

---

## üî¨ Experimentos Realizados

### 1. Chain-of-Thought (CoT)

**Acertijos utilizados:**
- **Acertijo 1:** Tres cofres con inscripciones (solo una verdadera)
- **Acertijo 2:** Tres personas con sombreros (red/blue)

**Objetivos:**
- Comparar razonamiento con y sin Chain-of-Thought
- Evaluar si CoT mejora precisi√≥n en problemas l√≥gicos

**Resultados clave:**
- Sin CoT: No lleg√≥ a conclusi√≥n en Acertijo 1
- Con CoT: Intent√≥ razonar pero lleg√≥ a respuesta incorrecta (Chest B)
- Con CoT en Acertijo 2: Respuesta correcta con mejor razonamiento

**Conclusi√≥n:** CoT estructura el razonamiento pero no garantiza correcci√≥n en l√≥gica compleja.

---

### 2. Tree-of-Thought (ToT)

**Problema:** Puzzle l√≥gico de 5 laboratorios de IA con 15 claves interdependientes

**Variantes experimentadas:**
1. **ToT Original:** Exploraci√≥n moderada con poda selectiva
2. **ToT + Verificaci√≥n L√≥gica:** Validaci√≥n exhaustiva en cada nodo
3. **ToT + Pruning Agresivo:** Poda inmediata de ramas sub√≥ptimas

**Resultados clave:**
- **ToT Original:** Explor√≥ 3 ramas, pod√≥ correctamente, pero se trunc√≥
- **ToT + Verificaci√≥n:** M√°s riguroso pero consumi√≥ tokens m√°s r√°pido
- **ToT + Pruning Agresivo:** FALL√ì - pod√≥ todas las ramas incluyendo la correcta

**Visualizaciones:** Dendrogramas comparativos muestran diferencias en ramificaci√≥n

**Conclusi√≥n:** ToT es poderoso pero requiere calibraci√≥n cuidadosa del pruning.

---

### 3. Self-Refine

**Casos experimentados:**

#### Caso 1: Infraestructura Cloud (3 data centers)
**Objetivo:** Dise√±ar plan de despliegue y balanceo de carga

**Proceso:**
- Iteraci√≥n 0: Soluci√≥n inicial
- Iteraci√≥n 1-2: Refinamientos basados en auto-evaluaci√≥n

**Resultado:** Mejora progresiva en completitud y detalle arquitect√≥nico

---

#### Caso 2: Experimento de Error Intencional
**Objetivo:** Probar si Self-Refine refuerza errores conceptuales

**Setup:** An√°lisis de crecimiento de usuarios con sesgo intencional del "manager"

**Resultados cr√≠ticos:**
- **Iteraci√≥n 0:** Acept√≥ asunci√≥n sin verificar independientemente
- **Iteraci√≥n 1:** Reforz√≥ la asunci√≥n con "checks" tautol√≥gicos
- **Iteraci√≥n 2:** Sigui√≥ reforzando, a√±adi√≥ "mejoras" superficiales

**Hallazgo importante:** Self-Refine NO corrigi√≥ error metodol√≥gico, sino que lo reforz√≥ con validaciones superficiales.

**Implicaci√≥n:** Self-Refine no es confiable para validar metodolog√≠a, solo para refinar ejecuci√≥n.

---

## üìä Resultados Principales

### Comparativa de T√©cnicas

| T√©cnica | Mejor para | Limitaci√≥n principal | Costo |
|---------|-----------|---------------------|-------|
| **CoT** | Problemas algor√≠tmicos | Sin recuperaci√≥n de errores | Bajo |
| **ToT** | Problemas complejos | Muy costoso (10-50x) | Alto |
| **Self-Refine** | Refinamiento iterativo | Refuerza errores conceptuales | Medio |

### Aplicaciones en Ing. Telem√°tica

- **CoT:** C√°lculo, f√≠sica, circuitos
- **ToT:** Dise√±o de redes, optimizaci√≥n, arquitecturas
- **Self-Refine:** Documentaci√≥n, ensayos, dise√±o de software

---

## üí° Hallazgos Clave

1. **CoT mejora estructura pero no garantiza correcci√≥n:** √ötil para transparencia, no para validaci√≥n

2. **ToT con pruning agresivo es arriesgado:** Puede eliminar todas las soluciones v√°lidas

3. **Self-Refine refuerza errores metodol√≥gicos:** Demostrado experimentalmente - solo mejora superficie, no fundamentos

4. **Limitaciones de contexto son reales:** 512 tokens fue suficiente para an√°lisis pero insuficiente para soluciones completas

5. **La t√©cnica debe coincidir con el problema:** No existe "mejor t√©cnica universal"

---

## üéì Aprendizajes

### Te√≥ricos
- Diferencias fundamentales entre CoT, ToT y Self-Refine
- C√≥mo cada t√©cnica maneja exploraci√≥n vs explotaci√≥n
- Importancia de feedback externo en auto-evaluaci√≥n

### Pr√°cticos
- Implementaci√≥n de LLMs en Colab con recursos limitados
- Dise√±o de prompts efectivos para cada t√©cnica
- An√°lisis cr√≠tico de outputs de LLMs

### Cr√≠ticos
- Los LLMs pueden generar respuestas coherentes pero incorrectas
- Auto-evaluaci√≥n sin verificaci√≥n externa es limitada
- Costo computacional es factor real en producci√≥n

---

## ‚ö†Ô∏è Limitaciones del Estudio

1. **Modelo √∫nico:** Solo Mistral 7B (resultados pueden variar con GPT-4, Claude, etc.)
2. **Tokens limitados:** 512 tokens trunc√≥ respuestas (intencional para an√°lisis)
3. **Problemas espec√≠ficos:** Acertijos l√≥gicos pueden no generalizar a todos los dominios
4. **Sin m√©tricas cuantitativas:** An√°lisis principalmente cualitativo
5. **Sin comparaci√≥n con humanos:** No hay baseline humano de performance

---

## üìö Referencias

### Papers Principales
1. Wei, J., et al. (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models". *NeurIPS 2022*
2. Yao, S., et al. (2023). "Tree of Thoughts: Deliberate Problem Solving with Large Language Models". *arXiv:2305.10601*
3. Madaan, A., et al. (2023). "Self-Refine: Iterative Refinement with Self-Feedback". *arXiv:2303.17651*
4. Huang, J., et al. (2023). "Large Language Models Cannot Self-Correct Reasoning Yet". *arXiv:2310.01798*

### Recursos Adicionales
- Anthropic Research: Constitutional AI and Self-Critique
- HuggingFace Transformers Documentation
- Mistral AI Official Documentation

---

## üöÄ C√≥mo Ejecutar

1. Abrir notebook en Google Colab
2. Configurar GPU: `Runtime > Change runtime type > GPU`
3. Ejecutar celdas en orden secuencial
4. Los experimentos generar√°n outputs y visualizaciones

**Nota:** Algunos experimentos pueden tardar 5-10 minutos debido a generaci√≥n de m√∫ltiples iteraciones.


